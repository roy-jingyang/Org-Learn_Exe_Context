{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "410f605b-0939-438a-8475-a6d42edad204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "e5693461-583f-43c0-acab-519b55452f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Case ID                  Activity Resource  \\\n",
      "0        Application_652823628      A_Create Application   User_1   \n",
      "1        Application_652823628               A_Submitted   User_1   \n",
      "2        Application_652823628                 A_Concept   User_1   \n",
      "3        Application_652823628                A_Accepted  User_52   \n",
      "4        Application_652823628            O_Create Offer  User_52   \n",
      "...                        ...                       ...      ...   \n",
      "475301  Application_1350494635                 O_Created  User_96   \n",
      "475302  Application_1350494635  O_Sent (mail and online)  User_96   \n",
      "475303  Application_1350494635                A_Complete  User_96   \n",
      "475304  Application_1350494635               A_Cancelled  User_28   \n",
      "475305  Application_1350494635               O_Cancelled  User_28   \n",
      "\n",
      "             Complete Timestamp at:event_origin            ct:loan_goal  \\\n",
      "0       2016-01-01 19:51:15.304     Application  Existing loan takeover   \n",
      "1       2016-01-01 19:51:15.352     Application  Existing loan takeover   \n",
      "2       2016-01-01 19:52:36.413     Application  Existing loan takeover   \n",
      "3       2016-01-02 21:23:04.299     Application  Existing loan takeover   \n",
      "4       2016-01-02 21:29:03.994           Offer  Existing loan takeover   \n",
      "...                         ...             ...                     ...   \n",
      "475301  2017-01-03 05:25:00.040           Offer        Home improvement   \n",
      "475302  2017-01-03 05:27:20.453           Offer        Home improvement   \n",
      "475303  2017-01-03 05:27:20.474     Application        Home improvement   \n",
      "475304  2017-01-16 19:51:21.114     Application        Home improvement   \n",
      "475305  2017-01-16 19:51:21.139           Offer        Home improvement   \n",
      "\n",
      "       ct:application_type  ct:requested_amount tt:month  tt:day tt:weekday  \\\n",
      "0               New credit              20000.0      Jan   Day_1        Fri   \n",
      "1               New credit              20000.0      Jan   Day_1        Fri   \n",
      "2               New credit              20000.0      Jan   Day_1        Fri   \n",
      "3               New credit              20000.0      Jan   Day_2        Sat   \n",
      "4               New credit              20000.0      Jan   Day_2        Sat   \n",
      "...                    ...                  ...      ...     ...        ...   \n",
      "475301          New credit              20000.0      Jan   Day_3        Tue   \n",
      "475302          New credit              20000.0      Jan   Day_3        Tue   \n",
      "475303          New credit              20000.0      Jan   Day_3        Tue   \n",
      "475304          New credit              20000.0      Jan  Day_16        Mon   \n",
      "475305          New credit              20000.0      Jan  Day_16        Mon   \n",
      "\n",
      "       tt:ampm  \n",
      "0           PM  \n",
      "1           PM  \n",
      "2           PM  \n",
      "3           PM  \n",
      "4           PM  \n",
      "...        ...  \n",
      "475301      AM  \n",
      "475302      AM  \n",
      "475303      AM  \n",
      "475304      PM  \n",
      "475305      PM  \n",
      "\n",
      "[440328 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load raw data & Preprocess DataFrame (enrich with derived attributes)\n",
    "log = 'bpic17'\n",
    "\n",
    "preprocess = False\n",
    "\n",
    "if preprocess:\n",
    "    fn = f'data/raw/{log}.csv'\n",
    "else:\n",
    "    fn = f'data/processed/{log}.csv'\n",
    "\n",
    "if preprocess:\n",
    "    if log == 'wabo':\n",
    "        df = pd.read_csv(fn)[[\n",
    "            'Case ID', 'Resource', 'Complete Timestamp',\n",
    "            'org:group', 'group',\n",
    "            'concept:name', 'responsible', 'department', 'channel'\n",
    "        ]]\n",
    "\n",
    "        df = df.rename(columns={\n",
    "            # Resource-related\n",
    "            'department': 'r:department',\n",
    "            'org:group': 'r:org:group',\n",
    "            'group': 'r:group',\n",
    "            # CT-related\n",
    "            'channel': 'ct:channel',\n",
    "            # AT-related\n",
    "            'concept:name': 'Activity',\n",
    "        })\n",
    "        \n",
    "        # filter meaningless values\n",
    "        df = df[~df['r:org:group'].isin(['EMPTY'])]\n",
    "        df = df[~df['r:group'].isin([''])]\n",
    "\n",
    "    if log == 'bpic17':\n",
    "        df = pd.read_csv(fn)[[\n",
    "            'Case ID', 'Activity', 'Resource', 'Complete Timestamp',\n",
    "            'EventOrigin', 'LoanGoal', 'ApplicationType', 'RequestedAmount'\n",
    "        ]]\n",
    "\n",
    "        df = df.rename(columns={\n",
    "            # Resource-related\n",
    "            # CT-related\n",
    "            'LoanGoal': 'ct:loan_goal', \n",
    "            'ApplicationType': 'ct:application_type', \n",
    "            'RequestedAmount': 'ct:requested_amount', \n",
    "            # AT-related\n",
    "            'EventOrigin': 'at:event_origin'\n",
    "        })\n",
    "        \n",
    "        # filter meaningless values\n",
    "        df = df[~df['ct:loan_goal'].isin(['Unknown'])]\n",
    "        \n",
    "\n",
    "    if log == 'bpic15':\n",
    "        df = pd.read_csv(fn)[[\n",
    "            'Case ID', 'Activity', 'Resource', 'Complete Timestamp',\n",
    "            '(case) last_phase', '(case) parts', 'action_code', 'municipality'\n",
    "        ]]\n",
    "        df = df.rename(columns={\n",
    "            # Resource-related\n",
    "            'municipality': 'r:municipality',\n",
    "            # CT-related\n",
    "            '(case) last_phase': 'ct:last_phase', \n",
    "            # AT-related\n",
    "        })\n",
    "        df = df.rename(columns={\n",
    "            '(case) parts': 'case_parts'\n",
    "        })\n",
    "        # TODO: derive 'ct:permit_type', 'at:phase'\n",
    "        df = df[~df['case_parts'].isna()]\n",
    "        df['ct:permit_type'] = df.apply(lambda row: 'Bouw' if 'Bouw' in str(row['case_parts']).split(',') else 'Non Bouw', axis=1)\n",
    "\n",
    "        # only look at the main subprocess: \"01_HOOFD\"\n",
    "        df = df[~df['action_code'].isna()]\n",
    "        df = df[df['action_code'].str.startswith('01_HOOFD')]\n",
    "        df['at:phase'] = df['action_code'].apply(lambda code: code[:10])\n",
    "        \n",
    "        # filter meaningless values\n",
    "\n",
    "    if log == 'bpic18':\n",
    "        pass\n",
    "\n",
    "    # Universal (on Disco outputs)\n",
    "    # derive and append TT related candidate attributes\n",
    "    df['Complete Timestamp'] = pd.to_datetime(df['Complete Timestamp'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    MONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    df['tt:month'] = df['Complete Timestamp'].apply(lambda ts: MONTHS[ts.month-1])\n",
    "    df['tt:day'] = df['Complete Timestamp'].apply(lambda ts: 'Day_{}'.format(ts.day))\n",
    "    WEEKDAYS = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    df['tt:weekday'] = df['Complete Timestamp'].apply(lambda ts: WEEKDAYS[ts.dayofweek])\n",
    "    df['tt:ampm'] = df['Complete Timestamp'].apply(lambda ts: 'AM' if ts.hour < 12 else 'PM')\n",
    "    \n",
    "    print(df)\n",
    "    df.to_csv(f'data/processed/{log}.csv')\n",
    "else:\n",
    "    df = pd.read_csv(fn, index_col=0)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "5df09159-a338-46d9-802b-a19d754924a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Shape: 144 x 2\n"
     ]
    }
   ],
   "source": [
    "if log == 'wabo':\n",
    "#     attr = 'Activity'\n",
    "#     attr = 'r:org:group'\n",
    "#     attr = 'r:group'\n",
    "#     attr = 'r:department'\n",
    "#     attr = 'ct:channel'\n",
    "#     attr = 'tt:month'\n",
    "#     attr = 'tt:day'\n",
    "#     attr = 'tt:weekday'\n",
    "    attr = 'tt:ampm'\n",
    "\n",
    "if log == 'bpic17':\n",
    "#     attr = 'Activity'\n",
    "#     attr = 'ct:loan_goal'\n",
    "#     attr = 'ct:application_type'\n",
    "#     #attr = 'ct:requested_amount'\n",
    "#     attr = 'at:event_origin'\n",
    "#     attr = 'tt:month'\n",
    "#     attr = 'tt:day'\n",
    "#     attr = 'tt:weekday'\n",
    "    attr = 'tt:ampm'\n",
    "\n",
    "if log == 'bpic15':\n",
    "#     attr = 'Activity'\n",
    "#     attr = 'r:municipality'\n",
    "#     attr = 'ct:last_phase'\n",
    "#     attr = 'ct:permit_type'\n",
    "#     attr = 'at:phase'\n",
    "#     attr = 'tt:month'\n",
    "#     attr = 'tt:day'\n",
    "#     attr = 'tt:weekday'\n",
    "    attr = 'tt:ampm'\n",
    "\n",
    "l = df.groupby(['Resource', attr]).size().groupby(level=0).size().to_numpy()\n",
    "print(l)\n",
    "avg_val_per_resource = l.mean()\n",
    "    \n",
    "df_grouped = df.groupby(['Resource', attr]).size().groupby(level=0).apply(lambda x: 100 * x / float(x.sum()))\n",
    "df_grouped = df_grouped.reset_index().pivot(index='Resource', columns=attr, values=0)\n",
    "#print(df_grouped)\n",
    "\n",
    "print(f'Shape: {len(df_grouped)} x {len(df_grouped.columns)}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#f, ax = plt.subplots(figsize=(20, 20))\n",
    "#ax = sns.heatmap(df_grouped.T, square=True, cbar=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "52e648fa-8931-4e62-a9e9-0e250807d76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Pairwise distance: \n",
      "0.5019425019425019\n",
      "1.986,0.502\n"
     ]
    }
   ],
   "source": [
    "# NOTE: the hopkins stat in package `pyclustertend` is defined similarly to https://en.wikipedia.org/wiki/Hopkins_statistic\n",
    "# Only that the complement is used, i.e., Hopkins = 1 - H, where H is calculated according to the definition shown on Wikipedia\n",
    "# Hence a value closer to 1 suggests strong clustering tendency\n",
    "from pyclustertend import vat, ivat, hopkins\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.spatial.distance import pdist\n",
    "from numpy import mean\n",
    "\n",
    "# scale\n",
    "X = scale(df_grouped.fillna(0).to_numpy())\n",
    "sample_size = int(0.2 * len(X))\n",
    "\n",
    "# binarize (for hacking hamming distance)\n",
    "B = (X > 0)\n",
    "\n",
    "'''\n",
    "# X-related\n",
    "# avg pdist\n",
    "avg_pdist = pdist(X).mean()\n",
    "print(f'Avg. Pairwise distance (Euclidean): \\n{avg_pdist}')\n",
    "# hopkins stat\n",
    "hopkins_stat = mean([hopkins(X, sampling_size=sample_size) for i in range(1000)])\n",
    "print(f'Hopkins statistic averaged over 1k runs, sampling {sample_size} / {len(X)} (20%) points: \\n{hopkins_stat}')\n",
    "'''\n",
    "\n",
    "# B-related\n",
    "# avg pdist\n",
    "avg_pdist_bin = pdist(B, metric='hamming').mean()\n",
    "print(f'Avg. Pairwise distance: \\n{avg_pdist_bin}')\n",
    "# hopkins stat\n",
    "#hopkins_stat_bin = mean([hopkins(B, sampling_size=sample_size) for i in range(1000)])\n",
    "#print(f'Hopkins statistic averaged over 1k runs, sampling {sample_size} / {len(B)} (20%) points: \\n{hopkins_stat_bin}')\n",
    "\n",
    "\n",
    "#print('{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}'.format(avg_val_per_resource, avg_pdist, hopkins_stat, avg_pdist_bin, hopkins_stat_bin))\n",
    "#print('{:.3f},{:.3f},{:.3f}'.format(avg_val_per_resource, avg_pdist_bin, hopkins_stat_bin))\n",
    "print('{:.3f},{:.3f}'.format(avg_val_per_resource, avg_pdist_bin))\n",
    "    \n",
    "#ivat(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87dfdd-39cc-4d86-97ec-a02b8de936ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
