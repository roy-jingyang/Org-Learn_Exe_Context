import sys

import pandas as pd

import ordinor.constants as const
from ordinor.io import read_csv
from ordinor.execution_context.rule_based import dispersal, impurity
from ordinor.org_model_miner.resource_features import direct_count
from ordinor.org_model_miner.group_discovery import ahc
from ordinor.org_model_miner.group_profiling import overall_score
from ordinor.conformance import fitness, precision

def evaluate_ec(rl):
    COLS = [const.CASE_TYPE, const.ACTIVITY_TYPE, const.TIME_TYPE]
    # m_event_co : pandas.Series
    #     An array indexed by event ids, recording labels of the execution
    #     contexts to which the events belong to.
    mat_event_co = rl[COLS]
    for col in COLS:
        codes, _ = pd.factorize(mat_event_co[col])
        mat_event_co.loc[:, col] = codes.astype(str)
    mat_event_co.loc[:, '_co'] = mat_event_co[COLS].agg('-'.join, axis=1)
    mat_event_co.loc[:, '_co'], _ = pd.factorize(mat_event_co['_co'])
    m_event_co = mat_event_co['_co']
    # m_co_t : pandas.DataFrame
    #     An array indexed by execution context ids, recording labels of
    #     the case types, activity types, and time types of execution
    #     contexts, i.e., the column number is 3.
    m_co_t = mat_event_co.drop_duplicates(subset='_co')
    m_co_t = m_co_t.set_index('_co')
    m_co_t = m_co_t.astype(int)
    # m_event_r : pandas.Series
    #     An array indexed by event ids, recording ids of the resources who
    #     originated the events.
    m_event_r = rl[const.RESOURCE]
    imp = impurity(m_event_co, m_event_r)
    dis = dispersal(m_co_t, m_event_co, m_event_r)
    return dis, imp

def disc_eval_om(rl, ct=None, at=None, tt=None, n_groups=2):
    if ct is not None and at is not None and tt is not None:
        print(f'CT={ct}, AT={at}, TT={tt}')
        # construct resource log by manually specifying attributes as types
        rl.loc[:, const.CASE_TYPE] = log[ct] if ct != 'NULL' else ct
        rl.loc[:, const.ACTIVITY_TYPE] = log[at] if at != 'NULL' else at
        rl.loc[:, const.TIME_TYPE] = log[tt] if tt != 'NULL' else tt
    
    # discover organizational model
    profiles = direct_count(rl, scale='normalize')
    rgs = ahc(profiles, n_groups=n_groups)
    om = overall_score(rgs, rl, auto_search=True)
    
    # evaluate organizational model
    fit = fitness(rl, om)
    prec = precision(rl, om)
    
    print('Fitness:\t{:.3f}'.format(fit))
    print('Precision:\t{:.3f}'.format(prec))
    
    return om, fit, prec
    
if __name__ == '__main__':
    print('*' * 20)
    # MODE = 'learn' / 'load'
    MODE = sys.argv[1]
    print('MODE: {}'.format(MODE))
    MAX_HEIGHT = int(sys.argv[2])
    print('MAX_HEIGHT: {}'.format(MAX_HEIGHT))
    if len(sys.argv) > 3:
        fn_miner = sys.argv[3]
        print('fn_miner: {}'.format(fn_miner))
    print('*' * 20)

    # Load data
    log = read_csv('./input/bpic2018.preprocessed.csv')

    # Filter out events with meaningless resource labels or without any resource label
    log = log[log[const.RESOURCE].notna()]
    log = log[~log[const.RESOURCE].isin(['0;n/a'])]

    # Filter out cases with missing value on required case attributes
    cases_rm = set()
    CT_ATTRS = [
        'case:department', 
        'case:redistribution', 'case:small farmer', 'case:young farmer', 
        'case:selected_risk', 'case:selected_manually', 'case:rejected'
    ]
    PENALTY_REASON_ATTRS = [col for col in log.columns if col.startswith('case:penalty_') and not col.startswith('case:penalty_amount')]
    CT_ATTRS += PENALTY_REASON_ATTRS
    for case, events in log.groupby(const.CASE_ID):
        for attr in CT_ATTRS:
            if events[attr].isna().all():
                cases_rm.add(case)
                break
    log = log[~log[const.CASE_ID].isin(cases_rm)]

    # Derive new columns based on the original ones
    # CT-related: N/A
    # AT-related: N/A
    # TT-related
    log.loc[:, 'year'] = log[const.TIMESTAMP].dt.year
    log.loc[:, 'month'] = log[const.TIMESTAMP].dt.month
    log.loc[:, 'weekday'] = log[const.TIMESTAMP].dt.day_name()

    # Derive execution contexts
    # 'NULL' denotes null type

    rl = log[[const.RESOURCE]]
    ct_cands = ['NULL'] + CT_ATTRS
    at_cands = ['NULL', 'activity', 'doctype', 'subprocess']
    tt_cands = ['NULL', 'year', 'month', 'weekday']

    '''
    # Evaluate execution contexts (generated by manually specifying types)
    results = []
    for (ct, at, tt) in product(ct_cands, at_cands, tt_cands):
        print(f'CT={ct}, AT={at}, TT={tt}')
        rl.loc[:, const.CASE_TYPE] = log[ct] if ct != 'NULL' else ct
        rl.loc[:, const.ACTIVITY_TYPE] = log[at] if at != 'NULL' else at
        rl.loc[:, const.TIME_TYPE] = log[tt] if tt != 'NULL' else tt
        dis, imp = evaluate_ec(rl)
        print('Dispersal:\t{:.3f}'.format(dis))
        print('Impurity:\t{:.3f}'.format(imp))
        results.append({'CT': ct, 'AT': at, 'TT': tt, 'dispersal': dis, 'impurity': imp})
    '''

    # Evaluate execution contexts (generated by ODT learning)
    from ordinor.execution_context import ODTMiner
    spec = {'type_def_attrs': {}}
    type_dim = ['CT', 'AT', 'TT']
    for i, l_type_def_attr in enumerate([ct_cands, at_cands, tt_cands]):
        for attr in l_type_def_attr:
            if attr != 'NULL':
                spec['type_def_attrs'][attr] = {'attr_type': 'categorical', 'attr_dim': type_dim[i]}

    if MODE == 'learn':
        miner = ODTMiner(log, spec, max_height=MAX_HEIGHT, trace_history=True)
    else:
        from ordinor.execution_context.base import BaseMiner
        with open(fn_miner, 'rb') as fin:
            miner = BaseMiner.from_file(fin)
    rl = miner.derive_resource_log(log)
    dis, imp = evaluate_ec(rl)
    print('Dispersal:\t{:.3f}'.format(dis))
    print('Impurity:\t{:.3f}'.format(imp))

    ts_now = pd.Timestamp.now()
    fn_miner = 'ODTMiner-bpic2018_h{}_{}.miner'.format(
        MAX_HEIGHT, 
        ts_now.strftime('%Y%m%d-%H%M%S')
    )
    if MODE == 'learn':
        print('Output miner to {}'.format(fn_miner))
        with open(fn_miner, 'wb') as fout:
            miner.to_file(fout)
    exit(1)

    # Discover and evaluate organizational models
    om, fit, prec = disc_eval_om(rl, ct='NULL', at=const.ACTIVITY, tt='month', n_groups=10)

    # Show organizational model
    for rg_id, rg in om.find_all_groups():
        print(rg)
        print('\t{}'.format(', '.join(sorted(om.find_group_members(rg_id)))))
        print('\t{}'.format(om.find_group_execution_contexts(rg_id)))
